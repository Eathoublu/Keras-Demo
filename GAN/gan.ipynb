{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN(Generative Adversarial Network) 生成对抗网络\n",
    "\n",
    "GAN由Ian J. Goodfellow在2015年提出，被认为是深度学习领域中最重要的发明之一，今天我们将实现一个最简单的GAN用来生成MNIST手写字符图片\n",
    "参考的材料有\n",
    "+ [GAN论文](https://arxiv.org/abs/1406.2661)\n",
    "+ [gan_mnist](https://github.com/jiemojiemo/deep-learning/blob/master/gan_mnist/Intro_to_GANs_Solution.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN简单介绍\n",
    "\n",
    "![GAN](https://raw.githubusercontent.com/jiemojiemo/deep-learning/66dc1a9946f082aeb8dbd16e3d105fd075bc8728/gan_mnist/assets/gan_diagram.png)\n",
    "\n",
    "GAN的由两部分组成：\n",
    "+ 生成器(Generator)，用来生成假(fake)的数据，在生成MNIST任务中，输入为任意噪声（通常为高斯噪声），输出为一张图片\n",
    "    >`Gaussuan noise --> Generator --> fake image`\n",
    "+ 判别器(Discriminator),用来判断数据是否为真(real)，在MNIST任务中，输入为一张图片，输出是为真的概率在[0,1]之间\n",
    "    >`fake/real image --> Discriminator --> probability of real`\n",
    "    \n",
    "这里有些例子，大概可以直观的解释一下GAN的工作原理\n",
    "\n",
    ">造假者和警察：造假者造出假钞，他们的目的是以假乱真，也就是使得假钞越来越像真钞；警察的目的是判断一张钞票是真的还是假的，尽可能使将一张真钞判断为真，避免判断的失误。\n",
    "\n",
    "> 魔术师和观众：魔术师能够变出一只假兔子，他需要确保这只兔子足够真实使得观众无法察觉出这是一只假兔子；而台下的观众总是希望能够找到魔术师的破绽，尽可能地判断魔术师变出来兔子的真假。\n",
    "\n",
    "以上，造假者和魔术师就是生成器，警察和观众就是判别器\n",
    "\n",
    "我们站在数学的角度来理解下GAN，论文中将GAN描述为一个\"two-player minimax game\"\n",
    "\n",
    "$$\n",
    "\\min_G \\max_D V(D,G) = \\Bbb E_{x \\sim p_{data}(x)}[\\log D(x)] + \\Bbb E_{z \\sim p_{z}(z)}[\\log(1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "这个部分我们分开两个部分看，首先是判别器，判别器D希望V的值**最大**\n",
    "$$\n",
    "\\max_D V(D,G) = \\Bbb E_{x \\sim p_{data}(x)}[\\log D(x)] + \\Bbb E_{z \\sim p_{z}(z)}[\\log(1 - D(G(z)))]\n",
    "$$\n",
    "其中\n",
    "1. $D(x) \\in [0,1]$, $\\log(D(x)) \\in [-\\infty ,0]$, $\\Bbb E_{x \\sim p_{data}(x)}[\\log D(x)] \\in [-\\infty, 0]$\n",
    "2. $1-D(G(z)) \\in [0,1]$, $\\log(1-D(G(z))) \\in [-\\infty, 0]$, $\\Bbb E_{z \\sim p_{z}(z)}[\\log(1 - D(G(z))) \\in [-\\infty, 0]$\n",
    "\n",
    "那么 $V(D,G) \\in [-\\infty, 0]$\n",
    "\n",
    "当$D(x)=1, D(G(z))=0$时 V的最大值为0，从这可以看出，GAN要求判别器能够\"明辨是非\"：\n",
    "+ 当 $x \\sim p_{data}$时（当 x 是real时），判别器要判断为真的概率越靠近1越好\n",
    "+ 当 $x \\sim p_{z}$时（当 x 是 fake时），判别器要判断为真的概率越靠近0越好\n",
    "\n",
    "然后我们看生成器，生成器G希望V的值**最小**(注意，只有后面那一项出现了G，所以下面的公式只有后面那一项)\n",
    "$$\n",
    "\\min_G V(D,G)= \\Bbb E_{z \\sim p_{z}(z)}[\\log(1 - D(G(z)))]\n",
    "$$\n",
    "其中\n",
    "1. $1-D(G(z)) \\in [0,1]$, $\\log(1-D(G(z))) \\in [-\\infty, 0]$, $\\Bbb E_{z \\sim p_{z}(z)}[\\log(1 - D(G(z))) \\in [-\\infty, 0]$\n",
    "\n",
    "那么 $V(D, G) \\in [-\\infty, 0]$\n",
    "\n",
    "当$D(G(z))=1$时，V的最小值为$-\\infty$，从这可以看出，GAN要求生成器欺骗判别器：\n",
    "+ 当 $x \\sim p_{z}$时（当 x 是fake）时，生成器要将x进行变化，使得让判别器判断x为真的概率越靠近1越好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN MNIST\n",
    "\n",
    "废话讲太多了，我们上代码，以下代码参考了 [gan_mnist](https://github.com/jiemojiemo/deep-learning/blob/master/gan_mnist/Intro_to_GANs_Solution.ipynb)\n",
    "\n",
    "原来的代码用的是Tensorflow，在这里我们将结合keras，因为keras封装好的各种层真的好方便啊\n",
    "\n",
    "这里使用MNIST数据集，我们将MNIST的数据拉成一维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student1/anaconda3/envs/keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型输入\n",
    "# 输入有两个：真实图像输入 和 高斯噪声输入\n",
    "def model_input(real_dim, z_dim):\n",
    "    '''\n",
    "    \n",
    "    Args:\n",
    "        real_dim, 真实图像的大小，例如MNIST中为 28*28 = 784\n",
    "        z_dim, 高斯噪声的大小\n",
    "        \n",
    "    Returns:\n",
    "        两个输入的张量\n",
    "    '''\n",
    "    \n",
    "    inputs_real = tf.placeholder(tf.float32, [None, real_dim], name='input_real')\n",
    "    inputs_z = tf.placeholder(tf.float32, [None, z_dim], name='input_z')\n",
    "    \n",
    "    return inputs_real, inputs_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络模型\n",
    "\n",
    "![Generator_network](https://raw.githubusercontent.com/jiemojiemo/deep-learning/66dc1a9946f082aeb8dbd16e3d105fd075bc8728/gan_mnist/assets/gan_network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, out_dim, n_units=128, reuse=False, alpha=0.01):\n",
    "    '''\n",
    "    生成网络\n",
    "    Args:\n",
    "        z, 高斯噪声输入\n",
    "        out_dim, 输出的大小，例如在MNIS中为MNIST\n",
    "        n_units, 中间隐藏层的单元个数\n",
    "        reuse, 是否重复使用\n",
    "        alpha, LeakyReLU的参数\n",
    "        \n",
    "    Returns:\n",
    "        生成的数据\n",
    "    '''\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        h1 = tf.keras.layers.Dense(n_units)(z)\n",
    "        h1 = tf.keras.layers.LeakyReLU(alpha)(h1)\n",
    "        \n",
    "        out = tf.keras.layers.Dense(out_dim, activation='tanh')(h1)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x, n_units=128, reuse=False, alpha=0.01):\n",
    "    '''\n",
    "    判别网络\n",
    "    Args:\n",
    "        x, 输入的图像\n",
    "        n_units, 中间隐藏层的单元个数\n",
    "        reuse, 是否重复使用\n",
    "        alpha, LeakyReLU的参数\n",
    "        \n",
    "    Returns:\n",
    "        为真的概率\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        h1 = tf.keras.layers.Dense(n_units)(x)\n",
    "        h1 = tf.keras.layers.LeakyReLU(alpha)(h1)\n",
    "        \n",
    "        logits = tf.keras.layers.Dense(1)(h1)\n",
    "        out = tf.keras.layers.Activation('softmax')(logits)\n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "\n",
    "# 28*28 for mnist\n",
    "input_size = 784\n",
    "# 高斯噪声输入大小\n",
    "z_size = 100\n",
    "# 隐层神经元个数\n",
    "g_hidden_size = 128\n",
    "d_hidden_size = 128\n",
    "# Leak factor\n",
    "alpha = 0.01\n",
    "# Smoothing，用于平滑label(后面有解释)\n",
    "smooth = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# 建立输入\n",
    "input_real, input_z = model_input(input_size, z_size)\n",
    "\n",
    "# 生成器\n",
    "g_model = generator(input_z, input_size, g_hidden_size, alpha=alpha)\n",
    "\n",
    "# 判别器\n",
    "d_model_real, d_logits_real = discriminator(input_real, d_hidden_size, alpha=alpha)\n",
    "d_model_fake, d_logits_fake = discriminator(g_model, reuse=True, n_units=d_hidden_size, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 判别器和生成器的Losses\n",
    "\n",
    "判别器的loss为 `d_loss = d_loss_real + d_loss_fake`, `d_loss_real`和`d_loss_fake`用的是二元交叉熵\n",
    "\n",
    "对于真实图像，给定的label全为1，但是作者认为1这个目标有点难，为了让网络更容易训练，把1降低为0.9\n",
    "对于假图像，给定的label全为0\n",
    "\n",
    "生成器的loss为 `g_loss`，生成器希望欺骗判别器，因此给定的label全为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nd_loss_real = tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(d_model_real)*(1-smooth), d_model_real))\\nd_loss_fake = tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.zeros_like(d_model_fake), d_model_fake))\\nd_loss = d_loss_real + d_loss_fake\\n\\ng_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(d_model_fake), d_model_fake))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "d_loss_real = tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(d_model_real)*(1-smooth), d_model_real))\n",
    "d_loss_fake = tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.zeros_like(d_model_fake), d_model_fake))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "g_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(d_model_fake), d_model_fake))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate losses\n",
    "\n",
    "d_loss_real = tf.reduce_mean(\n",
    "                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, \n",
    "                                                          labels=tf.ones_like(d_logits_real) * (1 - smooth)))\n",
    "d_loss_fake = tf.reduce_mean(\n",
    "                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, \n",
    "                                                          labels=tf.zeros_like(d_logits_real)))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "g_loss = tf.reduce_mean(\n",
    "             tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                     labels=tf.ones_like(d_logits_fake)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器\n",
    "生成器和判别器是独立的，因此在训练时我们用两个优化器分别对其训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "\n",
    "# 获取相应要训练的变量\n",
    "g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'generator')\n",
    "d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'discriminator')\n",
    "\n",
    "d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100... Discriminator Loss: 0.3538... Generator Loss: 4.1175\n",
      "Epoch 2/100... Discriminator Loss: 0.8443... Generator Loss: 1.2697\n",
      "Epoch 3/100... Discriminator Loss: 1.0744... Generator Loss: 11.1795\n",
      "Epoch 4/100... Discriminator Loss: 3.5870... Generator Loss: 1.4872\n",
      "Epoch 5/100... Discriminator Loss: 3.9371... Generator Loss: 0.5618\n",
      "Epoch 6/100... Discriminator Loss: 1.7787... Generator Loss: 0.8756\n",
      "Epoch 7/100... Discriminator Loss: 1.6583... Generator Loss: 1.8493\n",
      "Epoch 8/100... Discriminator Loss: 1.7657... Generator Loss: 1.5118\n",
      "Epoch 9/100... Discriminator Loss: 1.7330... Generator Loss: 1.0911\n",
      "Epoch 10/100... Discriminator Loss: 0.8375... Generator Loss: 4.1360\n",
      "Epoch 11/100... Discriminator Loss: 4.6800... Generator Loss: 0.7573\n",
      "Epoch 12/100... Discriminator Loss: 1.2294... Generator Loss: 1.0676\n",
      "Epoch 13/100... Discriminator Loss: 1.5328... Generator Loss: 1.1388\n",
      "Epoch 14/100... Discriminator Loss: 0.9903... Generator Loss: 1.4258\n",
      "Epoch 15/100... Discriminator Loss: 0.8329... Generator Loss: 2.2691\n",
      "Epoch 16/100... Discriminator Loss: 1.2667... Generator Loss: 1.0601\n",
      "Epoch 17/100... Discriminator Loss: 2.2882... Generator Loss: 1.4651\n",
      "Epoch 18/100... Discriminator Loss: 1.6866... Generator Loss: 0.8048\n",
      "Epoch 19/100... Discriminator Loss: 2.4278... Generator Loss: 0.5750\n",
      "Epoch 20/100... Discriminator Loss: 1.4950... Generator Loss: 1.7054\n",
      "Epoch 21/100... Discriminator Loss: 1.6795... Generator Loss: 0.9323\n",
      "Epoch 22/100... Discriminator Loss: 1.2144... Generator Loss: 1.2615\n",
      "Epoch 23/100... Discriminator Loss: 0.9326... Generator Loss: 1.9397\n",
      "Epoch 24/100... Discriminator Loss: 0.8579... Generator Loss: 1.5964\n",
      "Epoch 25/100... Discriminator Loss: 1.4347... Generator Loss: 1.3163\n",
      "Epoch 26/100... Discriminator Loss: 0.8733... Generator Loss: 2.3283\n",
      "Epoch 27/100... Discriminator Loss: 0.7541... Generator Loss: 2.3238\n",
      "Epoch 28/100... Discriminator Loss: 1.0125... Generator Loss: 2.0500\n",
      "Epoch 29/100... Discriminator Loss: 0.8154... Generator Loss: 2.1406\n",
      "Epoch 30/100... Discriminator Loss: 0.8091... Generator Loss: 2.1525\n",
      "Epoch 31/100... Discriminator Loss: 0.7377... Generator Loss: 2.5505\n",
      "Epoch 32/100... Discriminator Loss: 0.7732... Generator Loss: 2.5693\n",
      "Epoch 33/100... Discriminator Loss: 0.7815... Generator Loss: 1.9774\n",
      "Epoch 34/100... Discriminator Loss: 0.6147... Generator Loss: 3.3801\n",
      "Epoch 35/100... Discriminator Loss: 0.7376... Generator Loss: 2.9289\n",
      "Epoch 36/100... Discriminator Loss: 0.8045... Generator Loss: 2.7822\n",
      "Epoch 37/100... Discriminator Loss: 0.7972... Generator Loss: 2.2239\n",
      "Epoch 38/100... Discriminator Loss: 0.8200... Generator Loss: 2.4135\n",
      "Epoch 39/100... Discriminator Loss: 0.7288... Generator Loss: 2.7968\n",
      "Epoch 40/100... Discriminator Loss: 0.9216... Generator Loss: 2.2387\n",
      "Epoch 41/100... Discriminator Loss: 0.9786... Generator Loss: 2.2688\n",
      "Epoch 42/100... Discriminator Loss: 0.8033... Generator Loss: 2.2095\n",
      "Epoch 43/100... Discriminator Loss: 1.1496... Generator Loss: 1.2700\n",
      "Epoch 44/100... Discriminator Loss: 0.8764... Generator Loss: 2.0281\n",
      "Epoch 45/100... Discriminator Loss: 0.7739... Generator Loss: 2.9004\n",
      "Epoch 46/100... Discriminator Loss: 0.8294... Generator Loss: 2.4032\n",
      "Epoch 47/100... Discriminator Loss: 0.9188... Generator Loss: 1.9696\n",
      "Epoch 48/100... Discriminator Loss: 0.8603... Generator Loss: 2.1742\n",
      "Epoch 49/100... Discriminator Loss: 1.0327... Generator Loss: 1.5527\n",
      "Epoch 50/100... Discriminator Loss: 0.8739... Generator Loss: 1.7981\n",
      "Epoch 51/100... Discriminator Loss: 1.0058... Generator Loss: 2.0326\n",
      "Epoch 52/100... Discriminator Loss: 0.8656... Generator Loss: 1.8613\n",
      "Epoch 53/100... Discriminator Loss: 0.9269... Generator Loss: 2.0442\n",
      "Epoch 54/100... Discriminator Loss: 1.1257... Generator Loss: 1.6137\n",
      "Epoch 55/100... Discriminator Loss: 0.9126... Generator Loss: 2.0718\n",
      "Epoch 56/100... Discriminator Loss: 1.0079... Generator Loss: 2.0306\n",
      "Epoch 57/100... Discriminator Loss: 0.7718... Generator Loss: 2.3591\n",
      "Epoch 58/100... Discriminator Loss: 1.1817... Generator Loss: 1.7686\n",
      "Epoch 59/100... Discriminator Loss: 0.9554... Generator Loss: 1.7233\n",
      "Epoch 60/100... Discriminator Loss: 0.9868... Generator Loss: 1.6641\n",
      "Epoch 61/100... Discriminator Loss: 0.7914... Generator Loss: 2.2386\n",
      "Epoch 62/100... Discriminator Loss: 0.8517... Generator Loss: 2.0783\n",
      "Epoch 63/100... Discriminator Loss: 1.0914... Generator Loss: 1.6721\n",
      "Epoch 64/100... Discriminator Loss: 1.0067... Generator Loss: 1.7901\n",
      "Epoch 65/100... Discriminator Loss: 1.0461... Generator Loss: 1.4528\n",
      "Epoch 66/100... Discriminator Loss: 0.9204... Generator Loss: 1.5834\n",
      "Epoch 67/100... Discriminator Loss: 1.1032... Generator Loss: 1.7676\n",
      "Epoch 68/100... Discriminator Loss: 0.9180... Generator Loss: 2.0114\n",
      "Epoch 69/100... Discriminator Loss: 0.8860... Generator Loss: 1.9585\n",
      "Epoch 70/100... Discriminator Loss: 0.8688... Generator Loss: 1.6958\n",
      "Epoch 71/100... Discriminator Loss: 0.9521... Generator Loss: 1.7142\n",
      "Epoch 72/100... Discriminator Loss: 1.0509... Generator Loss: 1.6019\n",
      "Epoch 73/100... Discriminator Loss: 0.8954... Generator Loss: 1.5313\n",
      "Epoch 74/100... Discriminator Loss: 0.8907... Generator Loss: 2.1600\n",
      "Epoch 75/100... Discriminator Loss: 0.9976... Generator Loss: 1.5815\n",
      "Epoch 76/100... Discriminator Loss: 0.9450... Generator Loss: 2.2315\n",
      "Epoch 77/100... Discriminator Loss: 1.0138... Generator Loss: 1.4728\n",
      "Epoch 78/100... Discriminator Loss: 0.9060... Generator Loss: 1.7322\n",
      "Epoch 79/100... Discriminator Loss: 0.9082... Generator Loss: 1.7467\n",
      "Epoch 80/100... Discriminator Loss: 0.9085... Generator Loss: 1.7351\n",
      "Epoch 81/100... Discriminator Loss: 1.0228... Generator Loss: 1.7516\n",
      "Epoch 82/100... Discriminator Loss: 1.1906... Generator Loss: 1.4653\n",
      "Epoch 83/100... Discriminator Loss: 1.0534... Generator Loss: 1.8034\n",
      "Epoch 84/100... Discriminator Loss: 1.1030... Generator Loss: 1.3253\n",
      "Epoch 85/100... Discriminator Loss: 1.1252... Generator Loss: 1.4466\n",
      "Epoch 86/100... Discriminator Loss: 1.1002... Generator Loss: 1.7340\n",
      "Epoch 87/100... Discriminator Loss: 1.0699... Generator Loss: 1.7088\n",
      "Epoch 88/100... Discriminator Loss: 0.9922... Generator Loss: 1.5933\n",
      "Epoch 89/100... Discriminator Loss: 1.0116... Generator Loss: 1.3992\n",
      "Epoch 90/100... Discriminator Loss: 0.9860... Generator Loss: 1.6530\n",
      "Epoch 91/100... Discriminator Loss: 0.9349... Generator Loss: 1.5813\n",
      "Epoch 92/100... Discriminator Loss: 1.1043... Generator Loss: 1.4442\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 100\n",
    "samples = []\n",
    "losses = []\n",
    "\n",
    "# 只保存生成器变量\n",
    "saver = tf.train.Saver(var_list = g_vars)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for ii in range(mnist.train.num_examples // batch_size):\n",
    "        \n",
    "            # 获取一个batch的数据\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            # 将图片拉伸至一维\n",
    "            batch_images = batch[0].reshape((batch_size, 28*28))\n",
    "            # 预处理数据\n",
    "            batch_images = batch_images*2 - 1\n",
    "            \n",
    "            # 高斯噪声输入\n",
    "            batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "            \n",
    "            # run optimizers\n",
    "            _ = sess.run(d_train_opt, feed_dict={input_real:batch_images, input_z:batch_z})\n",
    "            _ = sess.run(g_train_opt, feed_dict={input_z:batch_z})\n",
    "            \n",
    "        train_loss_d = sess.run(d_loss, {input_z:batch_z, input_real:batch_images})\n",
    "        train_loss_g = g_loss.eval({input_z:batch_z})\n",
    "        \n",
    "        print(\"Epoch {}/{}...\".format(e+1, epochs),\n",
    "              \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "              \"Generator Loss: {:.4f}\".format(train_loss_g))\n",
    "        \n",
    "        # Save losses to view after training\n",
    "        losses.append((train_loss_d, train_loss_g))\n",
    "        \n",
    "        # Sample from generator as we're training for viewing afterwards\n",
    "        sample_z = np.random.uniform(-1, 1, size=(16, z_size))\n",
    "        gen_samples = sess.run(\n",
    "                       generator(input_z, input_size, reuse=True),\n",
    "                       feed_dict={input_z: sample_z})\n",
    "        \n",
    "        samples.append(gen_samples)\n",
    "        saver.save(sess, './checkpoints/generator.ckpt')\n",
    "        \n",
    "# Save training generator samples\n",
    "with open('train_samples.pkl', 'wb') as f:\n",
    "    pkl.dump(samples, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator')\n",
    "plt.plot(losses.T[1], label='Generator')\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_samples(epoch, samples):\n",
    "    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
    "    \n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load samples from generator taken while training\n",
    "with open('train_samples.pkl', 'rb') as f:\n",
    "    samples = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = view_samples(-1, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 10, 6\n",
    "fig, axes = plt.subplots(figsize=(7,12), nrows=rows, ncols=cols, sharex=True, sharey=True)\n",
    "\n",
    "for sample, ax_row in zip(samples[::int(len(samples)/rows)], axes):\n",
    "    for img, ax in zip(sample[::int(len(sample)/cols)], ax_row):\n",
    "        ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(var_list=g_vars)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    sample_z = np.random.uniform(-1, 1, size=(16, z_size))\n",
    "    gen_samples = sess.run(\n",
    "                   generator(input_z, input_size, reuse=True),\n",
    "                   feed_dict={input_z: sample_z})\n",
    "_ = view_samples(0, [gen_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
